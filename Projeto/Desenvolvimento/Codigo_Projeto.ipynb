{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.neighbors.kde module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from numpy import array, linspace\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "\n",
    "class OneDimensionalGaussianKernel():\n",
    "    def __init__(self, bandwidth=1):\n",
    "        self.bandwidth = bandwidth\n",
    "        self.n_components = 0\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.kde = KernelDensity(kernel='gaussian', bandwidth=self.bandwidth).fit(X)\n",
    "        self.line_min = X.min()\n",
    "        self.line_max = X.max()\n",
    "        s = linspace(self.line_min, self.line_max)\n",
    "        e = self.kde.score_samples(s.reshape(-1, 1))\n",
    "        self.mins, self.maxs = argrelextrema(e, np.less)[0], argrelextrema(e, np.greater)[0]\n",
    "        self.mins, self.maxs = s[self.mins], s[self.maxs]\n",
    "\n",
    "        # self.show()\n",
    "\n",
    "    def predict(self, X):\n",
    "        mins = []\n",
    "        mins.append(float('-inf'))\n",
    "        [mins.append(m) for m in self.mins]\n",
    "        mins.append(float('inf'))\n",
    "\n",
    "        predicted = []\n",
    "        for x in X:\n",
    "            index = -1\n",
    "            pre_min = None\n",
    "            for min in mins:\n",
    "                if pre_min is not None:\n",
    "                    if pre_min < x and x <= min:\n",
    "                        break\n",
    "\n",
    "                pre_min = min\n",
    "                index += 1\n",
    "\n",
    "            predicted.append(index)\n",
    "\n",
    "        self.n_components = max(predicted)+1\n",
    "\n",
    "        return predicted\n",
    "\n",
    "    def show(self):\n",
    "        s = linspace(self.line_min, self.line_max)\n",
    "        e = self.kde.score_samples(s.reshape(-1, 1))\n",
    "\n",
    "        # draw data\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.scatter(s, e, marker='o', s=25, edgecolor='k')\n",
    "        plt.show()\n",
    "\n",
    "        # find min and max\n",
    "        mi, ma = argrelextrema(e, np.less)[0], argrelextrema(e, np.greater)[0]\n",
    "        print(\"Minima:\", s[mi])\n",
    "        print(\"Maxima:\", s[ma])\n",
    "\n",
    "        plt.plot(s[:mi[0] + 1], e[:mi[0] + 1], 'r',\n",
    "                 s[mi[0]:mi[1] + 1], e[mi[0]:mi[1] + 1], 'g',\n",
    "                 s[mi[1]:], e[mi[1]:], 'b',\n",
    "                 s[ma], e[ma], 'go',\n",
    "                 s[mi], e[mi], 'ro')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, linspace\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "class MultiDimensionalGaussianKernel():\n",
    "    def __init__(self, bandwidth=1):\n",
    "        self.bandwidth = bandwidth\n",
    "        self.dict_variables = {}\n",
    "\n",
    "    def fit(self, X):\n",
    "        column_len = X.shape[1]\n",
    "        for i in range(column_len):\n",
    "            x = X[:, [i]]\n",
    "            gk = OneDimensionalGaussianKernel(self.bandwidth)\n",
    "            gk.fit(x)\n",
    "\n",
    "            self.dict_variables[i] = gk\n",
    "\n",
    "        for i, gk in self.dict_variables.items():\n",
    "            print(i, gk)\n",
    "\n",
    "    def predict(self, X):\n",
    "        column_len = X.shape[1]\n",
    "        predicted_list = []\n",
    "        for i in range(column_len):\n",
    "            x = X[:, [i]]\n",
    "            gk = self.dict_variables[i]\n",
    "            predicted = gk.predict(x)\n",
    "            predicted_list.append(predicted)\n",
    "\n",
    "        predicted = list(zip(*predicted_list))\n",
    "        predicted = [str(x) for x in predicted]\n",
    "\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import cluster, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "\n",
    "class Clustering_Alg:\n",
    "    def __init__(self):\n",
    "        self.datasets = []\n",
    "        self.selected_clustering_algorithms = []\n",
    "        self.clustering_algorithms = {}\n",
    "        self.clustering_variables = []\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def data_scaler(self, data):\n",
    "        temp_scaler = StandardScaler()\n",
    "        temp_scaler.mean_ = self.scaler.mean_\n",
    "        temp_scaler.var_ = self.scaler.var_\n",
    "        temp_scaler.n_samples_seen_ = self.scaler.n_samples_seen_\n",
    "        temp_scaler.scale_ = self.scaler.scale_\n",
    "        return temp_scaler.transform(data)\n",
    "\n",
    "    def set_algs(self, algs):\n",
    "        \"\"\"\n",
    "        :param algs: e.g., )'MiniBatchKMeans'\n",
    "                            'AffinityPropagation'\n",
    "                            'MeanShift'\n",
    "                            'SpectralClustering'\n",
    "                            'Ward'\n",
    "                            'AgglomerativeClustering'\n",
    "                            'DBSCAN'\n",
    "                            'OPTICS'\n",
    "                            'Birch'\n",
    "                            'GaussianMixture'\n",
    "                            'OneDGaussianKernel'\n",
    "        \"\"\"\n",
    "        self.selected_clustering_algorithms.append(algs)\n",
    "\n",
    "    def set_data(self, X_data, y_data, clustering_variables):\n",
    "        # data that was assigned as clustering_variables are used for clustering\n",
    "        self.clustering_variables = clustering_variables\n",
    "\n",
    "        self.X_data_all = X_data\n",
    "\n",
    "        X_clustring_data = X_data[clustering_variables]\n",
    "        y_clustring_data = y_data\n",
    "\n",
    "        data_cl = (  # First data set\n",
    "                        (  # X predictors [X1, X2]\n",
    "                            # e.g., ) np.array([[10, 20], [20, 30], [11, 11], [11, 24], [25, 36], [12, 11], [30, 20]]),\n",
    "                            X_clustring_data,\n",
    "                            # Y target [Y]\n",
    "                            # e.g., ) np.array([1, 0, 1, 1, 1, 0, 1])\n",
    "                            y_clustring_data\n",
    "                        ),\n",
    "                        {  # Algorithm Parameters\n",
    "                            # 'damping': 0.77, 'preference': -240,\n",
    "                            # 'quantile': 0.2, 'n_clusters': 2, 'min_samples': 20, 'xi': 0.25\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        self.datasets.append(data_cl)\n",
    "\n",
    "    def set_base(self, parameter):\n",
    "        n_clusters = 10\n",
    "        bandwidth = 0.05\n",
    "\n",
    "        if self.selected_clustering_algorithms[0] == 'OneDGaussianKernel' or \\\n",
    "           self.selected_clustering_algorithms[0] == 'MultiDGaussianKernel':\n",
    "            bandwidth = parameter\n",
    "        else:\n",
    "            n_clusters = parameter\n",
    "\n",
    "        self.default_base = {'quantile': .3,\n",
    "                             'eps': .3,\n",
    "                             'damping': .9,\n",
    "                             'preference': -200,\n",
    "                             'n_neighbors': 10,\n",
    "                             'n_clusters': n_clusters,\n",
    "                             'min_samples': 10,\n",
    "                             'xi': 0.05,\n",
    "                             'min_cluster_size': 0.1,\n",
    "                             'bandwidth':bandwidth}\n",
    "\n",
    "    def get_selected_clustering_alg(self):\n",
    "        return self.clustering_algorithms[self.selected_clustering_algorithms[0]]\n",
    "\n",
    "    def get_clustered_data(self, alg):\n",
    "        \"\"\"\n",
    "        This returns clustered data according to a selected algorithm\n",
    "        :param alg: a selected algorithm\n",
    "        :return: clustered data\n",
    "        \"\"\"\n",
    "\n",
    "        algorithm = self.clustering_algorithms[alg]\n",
    "        # print(self.datasets)\n",
    "        re_X_data = {}\n",
    "        for i_dataset, (dataset, algo_params) in enumerate(self.datasets):\n",
    "            X = dataset\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                for i in range(len(X.index)):\n",
    "                    data_id = algorithm.labels_[i]\n",
    "                    if data_id not in re_X_data:\n",
    "                        re_X_data[data_id] = X.iloc[i].transpose()\n",
    "                    else:\n",
    "                        re_X_data[data_id] = pd.concat((re_X_data[data_id], X.iloc[i]), axis=1)\n",
    "\n",
    "                for key in re_X_data:\n",
    "                    re_X_data[key] = re_X_data[key].transpose()\n",
    "\n",
    "            elif not isinstance(X, pd.DataFrame):\n",
    "                for i in range(len(X)):\n",
    "                    data_id = algorithm.labels_[i]\n",
    "                    if data_id not in re_X_data:\n",
    "                        re_X_data[data_id] = np.array([X[i]])\n",
    "                    else:\n",
    "                        re_X_data[data_id] = np.concatenate((re_X_data[data_id], [X[i]]), axis=0)\n",
    "\n",
    "            return re_X_data\n",
    "\n",
    "    def get_clustered_data_XY(self, alg):\n",
    "        \"\"\"\n",
    "        This returns clustered data according to a selected algorithm\n",
    "        :param alg: a selected algorithm\n",
    "        :return: clustered data\n",
    "        \"\"\"\n",
    "\n",
    "        algorithm = self.clustering_algorithms[alg]\n",
    "        # print(self.datasets)\n",
    "        re_X_data = {}\n",
    "        re_y_data = {}\n",
    "        re_data = {}\n",
    "        for i_dataset, (dataset, algo_params) in enumerate(self.datasets):\n",
    "            X, y = dataset\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                for i in range(len(X.index)):\n",
    "                    data_id = algorithm.labels_[i]\n",
    "                    if data_id not in re_X_data:\n",
    "                        re_X_data[data_id] = self.X_data_all.iloc[i].transpose()\n",
    "                        re_y_data[data_id] = y.iloc[i]\n",
    "                        re_data[data_id] = pd.concat([self.X_data_all.iloc[i], y.iloc[i]], axis=0)\n",
    "                    else:\n",
    "                        re_X_data[data_id] = pd.concat((re_X_data[data_id], self.X_data_all.iloc[i]), axis=1)\n",
    "                        re_y_data[data_id] = pd.concat((re_y_data[data_id], y.iloc[i]), axis=1)\n",
    "                        re_data[data_id] = pd.concat((re_X_data[data_id], re_y_data[data_id]), axis=0)\n",
    "\n",
    "                for key in re_X_data:\n",
    "                    re_X_data[key] = re_X_data[key].transpose()\n",
    "                    re_y_data[key] = re_y_data[key].transpose()\n",
    "                    re_data[key] = re_data[key].transpose()\n",
    "\n",
    "            elif not isinstance(X, pd.DataFrame):\n",
    "                for i in range(len(X)):\n",
    "                    data_id = algorithm.labels_[i]\n",
    "                    if data_id not in re_X_data:\n",
    "                        re_X_data[data_id] = np.array([self.X_data_all[i]])\n",
    "                        re_y_data[data_id] = np.array([y[i]])\n",
    "                        re_data[data_id] = np.concatenate((re_X_data[data_id], re_y_data[data_id]), axis=1)\n",
    "                    else:\n",
    "                        re_X_data[data_id] = np.concatenate((re_X_data[data_id], [self.X_data_all[i]]), axis=0)\n",
    "                        re_y_data[data_id] = np.concatenate((re_y_data[data_id], [y[i]]), axis=0)\n",
    "                        re_data[data_id] = np.concatenate((re_X_data[data_id], re_y_data[data_id]), axis=1)\n",
    "\n",
    "            return re_data, re_X_data, re_y_data\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        for i_dataset, (dataset, algo_params) in enumerate(self.datasets):\n",
    "            # update parameters with dataset-specific values\n",
    "            params = self.default_base.copy()\n",
    "            params.update(algo_params)\n",
    "\n",
    "            X, y = dataset\n",
    "\n",
    "            # normalize dataset for easier parameter selection\n",
    "            X = self.scaler.fit_transform(X)\n",
    "\n",
    "            print(f'mean{self.scaler.mean_}, var{self.scaler.var_}, n_samples[{self.scaler.n_samples_seen_}], scale[{self.scaler.scale_}]')\n",
    "\n",
    "            # estimate bandwidth for mean shift\n",
    "            bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "            # connectivity matrix for structured Ward\n",
    "            connectivity = kneighbors_graph(X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "\n",
    "            # make connectivity symmetric\n",
    "            connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "            # ============\n",
    "            # Create cluster objects\n",
    "            # ============\n",
    "            for alg in self.selected_clustering_algorithms:\n",
    "                if alg is 'MiniBatchKMeans':\n",
    "                    self.clustering_algorithms['MiniBatchKMeans'] = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "                elif alg is 'AffinityPropagation':\n",
    "                    self.clustering_algorithms['AffinityPropagation'] = cluster.AffinityPropagation(damping=params['damping'],preference=params['preference'])\n",
    "                elif alg is 'MeanShift':\n",
    "                    self.clustering_algorithms['MeanShift'] = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "                elif alg is 'SpectralClustering':\n",
    "                    self.clustering_algorithms['SpectralClustering'] = cluster.SpectralClustering(n_clusters=params['n_clusters'], eigen_solver='arpack', affinity=\"nearest_neighbors\")\n",
    "                elif alg is 'Ward':\n",
    "                    self.clustering_algorithms['Ward'] = cluster.AgglomerativeClustering(n_clusters=params['n_clusters'], linkage='ward', connectivity=connectivity)\n",
    "                elif alg is 'AgglomerativeClustering':\n",
    "                    self.clustering_algorithms['AgglomerativeClustering'] = cluster.AgglomerativeClustering(linkage=\"average\", affinity=\"cityblock\", n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "                elif alg is 'DBSCAN':\n",
    "                    self.clustering_algorithms['DBSCAN'] = cluster.DBSCAN(eps=params['eps'])\n",
    "                elif alg is 'OPTICS':\n",
    "                    self.clustering_algorithms['OPTICS'] = cluster.OPTICS(min_samples=params['min_samples'], xi=params['xi'], min_cluster_size=params['min_cluster_size'])\n",
    "                elif alg is 'Birch':\n",
    "                    self.clustering_algorithms['Birch'] = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "                elif alg is 'GaussianMixture':\n",
    "                    self.clustering_algorithms['GaussianMixture'] = mixture.GaussianMixture(n_components=params['n_clusters'], covariance_type='full')\n",
    "                elif alg is 'OneDGaussianKernel':\n",
    "                    self.clustering_algorithms['OneDGaussianKernel'] = OneDimensionalGaussianKernel(bandwidth=params['bandwidth'])\n",
    "                elif alg is 'MultiDGaussianKernel':\n",
    "                    self.clustering_algorithms['MultiDGaussianKernel'] = MultiDimensionalGaussianKernel(bandwidth=params['bandwidth'])\n",
    "\n",
    "\n",
    "            # self.clustering_algorithms['MiniBatchKMeans'] = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "            # self.clustering_algorithms['AffinityPropagation'] = cluster.AffinityPropagation(damping=params['damping'], preference=params['preference'])\n",
    "            # self.clustering_algorithms['MeanShift'] = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "            # self.clustering_algorithms['SpectralClustering'] = cluster.SpectralClustering(n_clusters=params['n_clusters'], eigen_solver='arpack', affinity=\"nearest_neighbors\")\n",
    "            # self.clustering_algorithms['Ward'] = cluster.AgglomerativeClustering(n_clusters=params['n_clusters'], linkage='ward', connectivity=connectivity)\n",
    "            # self.clustering_algorithms['AgglomerativeClustering'] = cluster.AgglomerativeClustering(linkage=\"average\", affinity=\"cityblock\", n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "            # self.clustering_algorithms['DBSCAN'] = cluster.DBSCAN(eps=params['eps'])\n",
    "            # self.clustering_algorithms['OPTICS'] = cluster.OPTICS(min_samples=params['min_samples'], xi=params['xi'], min_cluster_size=params['min_cluster_size'])\n",
    "            # self.clustering_algorithms['Birch'] = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "            # self.clustering_algorithms['GaussianMixture'] = mixture.GaussianMixture(n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "            for name, algorithm in self.clustering_algorithms.items():\n",
    "                t0 = time.time()\n",
    "\n",
    "                # catch warnings related to kneighbors_graph\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\n",
    "                        \"ignore\",\n",
    "                        message=\"the number of connected components of the \" +\n",
    "                        \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                        \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                        category=UserWarning)\n",
    "                    warnings.filterwarnings(\n",
    "                        \"ignore\",\n",
    "                        message=\"Graph is not fully connected, spectral embedding\" +\n",
    "                        \" may not work as expected.\",\n",
    "                        category=UserWarning)\n",
    "                    algorithm.fit(X)\n",
    "\n",
    "                t1 = time.time()\n",
    "                if hasattr(algorithm, 'labels_'):\n",
    "                    y_pred = algorithm.labels_.astype(np.int)\n",
    "                else:\n",
    "                    y_pred = algorithm.predict(X)\n",
    "                    algorithm.labels_ = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "\n",
    "class cluster_test:\n",
    "    def __init__(self):\n",
    "        self.datasets = []\n",
    "\n",
    "    def set_data(self, X_data, y_data):\n",
    "        new_data = (  # First data set\n",
    "                        (  # X predictors [X1, X2]\n",
    "                            # e.g., ) np.array([[10, 20], [20, 30], [11, 11], [11, 24], [25, 36], [12, 11], [30, 20]]),\n",
    "                            X_data,\n",
    "                            # Y target [Y]\n",
    "                            # e.g., ) np.array([1, 0, 1, 1, 1, 0, 1])\n",
    "                            y_data\n",
    "                        ),\n",
    "                        {  # Algorithm Parameters\n",
    "                            # 'damping': 0.77, 'preference': -240,\n",
    "                            # 'quantile': 0.2, 'n_clusters': 2, 'min_samples': 20, 'xi': 0.25\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        self.datasets.append(new_data)\n",
    "\n",
    "        self.default_base = {'quantile': .3,\n",
    "                             'eps': .3,\n",
    "                             'damping': .9,\n",
    "                             'preference': -200,\n",
    "                             'n_neighbors': 10,\n",
    "                             'n_clusters': 5,\n",
    "                             'min_samples': 10,\n",
    "                             'xi': 0.05,\n",
    "                             'min_cluster_size': 0.1}\n",
    "\n",
    "    def run(self):\n",
    "        plt.figure(figsize=(9 * 2 + 3, len(self.datasets)*2))\n",
    "        # plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05, hspace=.01)\n",
    "\n",
    "        plot_num = 1\n",
    "\n",
    "        for i_dataset, (dataset, algo_params) in enumerate(self.datasets):\n",
    "            # update parameters with dataset-specific values\n",
    "            params = self.default_base.copy()\n",
    "            params.update(algo_params)\n",
    "\n",
    "            X, y = dataset\n",
    "\n",
    "            # normalize dataset for easier parameter selection\n",
    "            X = StandardScaler().fit_transform(X)\n",
    "\n",
    "            # estimate bandwidth for mean shift\n",
    "            bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "            # connectivity matrix for structured Ward\n",
    "            connectivity = kneighbors_graph(X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "\n",
    "            # make connectivity symmetric\n",
    "            connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "            # ============\n",
    "            # Create cluster objects\n",
    "            # ============\n",
    "            ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "            two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "            ward = cluster.AgglomerativeClustering(n_clusters=params['n_clusters'], linkage='ward', connectivity=connectivity)\n",
    "            spectral = cluster.SpectralClustering(n_clusters=params['n_clusters'], eigen_solver='arpack', affinity=\"nearest_neighbors\")\n",
    "            dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "            optics = cluster.OPTICS(min_samples=params['min_samples'], xi=params['xi'], min_cluster_size=params['min_cluster_size'])\n",
    "            affinity_propagation = cluster.AffinityPropagation(damping=params['damping'], preference=params['preference'])\n",
    "            average_linkage = cluster.AgglomerativeClustering(linkage=\"average\", affinity=\"cityblock\", n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "            birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "            gmm = mixture.GaussianMixture(n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "            clustering_algorithms = (\n",
    "                ('MiniBatchKMeans', two_means),\n",
    "                ('AffinityPropagation', affinity_propagation),\n",
    "                ('MeanShift', ms),\n",
    "                ('SpectralClustering', spectral),\n",
    "                ('Ward', ward),\n",
    "                ('AgglomerativeClustering', average_linkage),\n",
    "                ('DBSCAN', dbscan),\n",
    "                ('OPTICS', optics),\n",
    "                ('Birch', birch),\n",
    "                ('GaussianMixture', gmm)\n",
    "            )\n",
    "\n",
    "            for name, algorithm in clustering_algorithms:\n",
    "                t0 = time.time()\n",
    "\n",
    "                # catch warnings related to kneighbors_graph\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\n",
    "                        \"ignore\",\n",
    "                        message=\"the number of connected components of the \" +\n",
    "                        \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                        \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                        category=UserWarning)\n",
    "                    warnings.filterwarnings(\n",
    "                        \"ignore\",\n",
    "                        message=\"Graph is not fully connected, spectral embedding\" +\n",
    "                        \" may not work as expected.\",\n",
    "                        category=UserWarning)\n",
    "                    algorithm.fit(X)\n",
    "\n",
    "                t1 = time.time()\n",
    "                if hasattr(algorithm, 'labels_'):\n",
    "                    y_pred = algorithm.labels_.astype(np.int)\n",
    "                else:\n",
    "                    y_pred = algorithm.predict(X)\n",
    "\n",
    "                plt.subplot(len(self.datasets), len(clustering_algorithms), plot_num)\n",
    "                if i_dataset == 0:\n",
    "                    plt.title(name, size=9)\n",
    "\n",
    "                colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                                     '#f781bf', '#a65628', '#984ea3',\n",
    "                                                     '#999999', '#e41a1c', '#dede00']),\n",
    "                                              int(max(y_pred) + 1))))\n",
    "                # add black color for outliers (if any)\n",
    "                colors = np.append(colors, [\"#000000\"])\n",
    "                plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "                plt.xlim(-2.5, 2.5)\n",
    "                plt.ylim(-2.5, 2.5)\n",
    "                plt.xticks(())\n",
    "                plt.yticks(())\n",
    "                plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                         transform=plt.gca().transAxes, size=8,\n",
    "                         horizontalalignment='right')\n",
    "                plot_num += 1\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "class DMP_runner():\n",
    "    def __init__(self, output):\n",
    "        self.output = output\n",
    "        self.my_df = pd.DataFrame()\n",
    "        self.__model = ''\n",
    "        self.__evidence = ''\n",
    "\n",
    "    def setModel(self, sbn):\n",
    "        with open(sbn, 'r') as file:\n",
    "            self.__model += file.read()\n",
    "\n",
    "    def addEvidence(self, nodeName, value):\n",
    "        s = '\\ndefineEvidence({}, {});'.format(nodeName, value)\n",
    "        self.__evidence += s\n",
    "\n",
    "    def runDMP(self):\n",
    "        model = self.__model + self.__evidence + '\\nrun(DMP);'\n",
    "        self.run(model)\n",
    "        self.__evidence = ''\n",
    "\n",
    "    def saveResult(self):\n",
    "        ###############################\n",
    "        # Save reasoned results\n",
    "        nodes = []\n",
    "        with open(self.output) as json_file:\n",
    "            net = json.load(json_file)\n",
    "            for node in net:\n",
    "                for obj, contents in node.items():\n",
    "                    nodes.append(obj)\n",
    "\n",
    "        df = pd.DataFrame(columns=nodes)\n",
    "\n",
    "        row_data = {}\n",
    "        with open(self.output) as json_file:\n",
    "            net = json.load(json_file)\n",
    "            for node in net:\n",
    "                for obj, contents in node.items():\n",
    "                    if 'marginal' in contents:\n",
    "                        row_data[obj] = contents['marginal']['MU']\n",
    "                    elif 'evidence' in contents:\n",
    "                        row_data[obj] = contents['evidence']['mean']\n",
    "\n",
    "        df = df.append(row_data, ignore_index=True)\n",
    "        self.my_df = self.my_df.append(df, ignore_index=True)\n",
    "        return self.my_df\n",
    "\n",
    "    def run(self, model):\n",
    "        # create a hybrid model script to be executed in dmp.jar\n",
    "        cwd = os.path.dirname(os.path.realpath(__file__))\n",
    "        dmp_args = \" -b \"\n",
    "        dmp_args += \"\\\"\" + model + \"\\\"\"\n",
    "        dmp_args += \" -p \"\n",
    "        dmp_args += \"\\\"\" + self.output + \"\\\"\"\n",
    "\n",
    "        # supervisor call to DMP (command line)\n",
    "        proc = subprocess.Popen(\"java -jar \" + cwd + \"/dmp.jar\" + dmp_args)\n",
    "\n",
    "        proc.wait()\n",
    "\n",
    "        (stdout, stderr) = proc.communicate()\n",
    "\n",
    "        if proc.returncode != 0:\n",
    "            print(stderr)\n",
    "            sys.exit()\n",
    "        else:\n",
    "            # print(\"success\")\n",
    "            pass\n",
    "\n",
    "    ##############################################################\n",
    "    # sampling\n",
    "    def sampling(self, sbn, size):\n",
    "        time.sleep(0.1)\n",
    "        model = \"\"\n",
    "\n",
    "        with open(sbn, 'r') as file:\n",
    "            model += file.read()\n",
    "        model += \" run(LW, arg(1,1));\"\n",
    "\n",
    "        for i in range(size):\n",
    "            print(\"   Sampling {:f} % ********************************\".format(100*i/size))\n",
    "\n",
    "            # Run a BN once using LW\n",
    "            self.run(model)\n",
    "\n",
    "        return self.saveResult()\n",
    "\n",
    "    def sampling_by_thread(self, sbn, size, size_thread=50):\n",
    "        # size_thread = 4\n",
    "        distributed_size = size\n",
    "        ts = time.time()\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=size_thread) as executor:\n",
    "            executor.map(self.sampling(sbn, distributed_size), range(size_thread))\n",
    "\n",
    "        # with concurrent.futures.ProcessPoolExecutor(max_workers=size_thread) as executor:\n",
    "        #     executor.map(self.sampling(sbn, distributed_size), range(size_thread))\n",
    "\n",
    "        print(\"== sampling_by_thread end: Time {} ==============================\".format(time.time() - ts))\n",
    "\n",
    "        return self.my_df\n",
    "\n",
    "    ##############################################################\n",
    "    # read output from DMP\n",
    "    def read_results(self, data_file_name):\n",
    "        with open(data_file_name) as json_file:\n",
    "            net = json.load(json_file)\n",
    "            for node in net:\n",
    "                for obj, contents in node.items():\n",
    "                    print('node: ' + obj)\n",
    "                    for attr, values in contents.items():\n",
    "                        print(\" \" + attr + ': ' + str(values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "class HML_runner():\n",
    "    def __init__(self, model_structure=None):\n",
    "        self.model_structure = model_structure\n",
    "        self.csv = \"\"\n",
    "        self.output = \"\"\n",
    "\n",
    "    def run(self, csv, output):\n",
    "        self.csv = csv\n",
    "        self.output = output\n",
    "\n",
    "        # create a hybrid model script to be executed in dmp.jar\n",
    "        cwd = os.path.dirname(os.path.realpath(__file__))\n",
    "        dmp_args = \" -b \"\n",
    "        dmp_args += \"\\\"\" + self.model_structure + \"\\\"\"\n",
    "        dmp_args += \" -c \"\n",
    "        dmp_args += \"\\\"\" + self.csv + \"\\\"\"\n",
    "        dmp_args += \" -p \"\n",
    "        dmp_args += \"\\\"\" + self.output + \"\\\"\"\n",
    "\n",
    "        # supervisor call to HML (command line)\n",
    "        # print(\"current working directory: \" + cwd)\n",
    "        proc = subprocess.Popen(\"java -jar \" + cwd + \"/HML.jar\" + dmp_args)\n",
    "\n",
    "        proc.wait()\n",
    "\n",
    "        (stdout, stderr) = proc.communicate()\n",
    "\n",
    "        if proc.returncode != 0:\n",
    "            print(stderr)\n",
    "            sys.exit()\n",
    "        else:\n",
    "            # print(\"success\")\n",
    "            with open(self.output, 'r') as file:\n",
    "                self.model_learned = file.read()\n",
    "            return self.model_learned\n",
    "\n",
    "    def make_Model(self, child, parents):\n",
    "        self.model_structure = \"\"\n",
    "        for nodeName in parents:\n",
    "            self.model_structure += self.creat_BN_parent_node(nodeName)\n",
    "        self.model_structure += self.creat_BN_child_Node(child, parents)\n",
    "        # print(self.model_structure)\n",
    "        return self.model_structure\n",
    "\n",
    "    def creat_BN_parent_node(self, nodeName):\n",
    "        s = \"defineNode({}, des);\".format(nodeName)\n",
    "        s += \"{ defineState(Continuous);\"\n",
    "        s += \"  p( {} ) = NormalDist(  0, 0.000001 );\".format(nodeName)\n",
    "        s += \"}\"\n",
    "        return s\n",
    "\n",
    "    def creat_BN_child_Node(self, nodeName, parents):\n",
    "        p = \", \".join(parents)\n",
    "        e = \"+ \".join(parents)\n",
    "        s = \"defineNode({}, des);\".format(nodeName)\n",
    "        s += \"{ defineState(Continuous);\"\n",
    "        s += \"  p( {} | {} ) = {} + NormalDist(  0, 0.000001 );\".format(nodeName, p, e)\n",
    "        s += \"}\"\n",
    "        return s\n",
    "\n",
    "    # read output from DMP\n",
    "    def read_results(self, data_file_name):\n",
    "        with open(data_file_name) as json_file:\n",
    "            net = json.load(json_file)\n",
    "            for node in net:\n",
    "                for obj, contents in node.items():\n",
    "                    print('node: ' + obj)\n",
    "                    for attr, values in contents.items():\n",
    "                        print(\" \" + attr + ': ' + str(values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "class RegressionML():\n",
    "    def __init__(self, metrics='R2'):\n",
    "        # Set metrics\n",
    "        if metrics == 'R2':\n",
    "            self.score = r2_score\n",
    "        elif metrics == 'MAE':\n",
    "            self.score = mean_absolute_error\n",
    "\n",
    "    def show_results(self, y_predicted, y_actual=None, ML_Alg=None, cv=5):\n",
    "        if y_actual is None or len(y_actual) == 0:\n",
    "            return 0\n",
    "\n",
    "        # Score metric: R^2\n",
    "        \"\"\" The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
    "            sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
    "            sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
    "            The best possible score is 1.0 and it can be negative (because the\n",
    "            model can be arbitrarily worse). A constant model that always\n",
    "            predicts the expected value of y, disregarding the input features,\n",
    "            would get a R^2 score of 0.0.\"\"\"\n",
    "\n",
    "        # Training Cross Validation Accuracy Score\n",
    "        # if ML_Alg != None:\n",
    "        #     val_scores = cross_val_score(ML_Alg, X_train, np.ravel(y_train), cv=cv)\n",
    "        #     print(f'Training Cross Validation Score({val_scores.mean()}):', val_scores)\n",
    "\n",
    "        # Testing Accuracy Score\n",
    "        r2 = self.score(y_actual, y_predicted)\n",
    "        # if len(y_actual) > 0 and len(y_predicted) > 0:\n",
    "        #     print(\"Testing R^2 Accuracy Score:\", r2)\n",
    "\n",
    "        return r2\n",
    "\n",
    "    def DecisionTreeClassifier_run(self, X_train, y_train, X_test, y_actual=None):\n",
    "        ml_model = self.DecisionTreeClassifier_ML(X_train, y_train)\n",
    "        return self.prediction(ml_model, X_test, y_actual)\n",
    "\n",
    "    def DecisionTreeClassifier_ML(self, X_train, y_train):\n",
    "        # print(\"*** Decision Tree Regression ***\")\n",
    "        ML_Alg = DecisionTreeClassifier(max_depth=6)\n",
    "        ml_model = ML_Alg.fit(X_train, y_train.ravel())\n",
    "        return ml_model\n",
    "\n",
    "    def GaussianNB_run(self, X_train, y_train, X_test, y_actual=None):\n",
    "        ml_model = self.GaussianNB_ML(X_train, y_train)\n",
    "        return self.prediction(ml_model, X_test, y_actual)\n",
    "\n",
    "    def GaussianNB_ML(self, X_train, y_train):\n",
    "        # print(\"*** Gaussian NB ***\")\n",
    "        ML_Alg = GaussianNB()\n",
    "        ml_model = ML_Alg.fit(X_train, np.ravel(y_train))\n",
    "        return ml_model\n",
    "\n",
    "    def RandomForestRegressor_run(self, X_train, y_train, X_test, y_actual=None):\n",
    "        ml_model = self.RandomForestRegressor_ML(X_train, y_train)\n",
    "        return self.prediction(ml_model, X_test, np.ravel(y_actual))\n",
    "\n",
    "    def RandomForestRegressor_ML(self, X_train, y_train):\n",
    "        # print(\"*** Random Forest Regression ***\")\n",
    "        ML_Alg = RandomForestRegressor(n_estimators=100)\n",
    "        ml_model = ML_Alg.fit(X_train, np.ravel(y_train))\n",
    "        return ml_model\n",
    "\n",
    "\n",
    "    def LinearRegressor_run(self, X_train, y_train, X_test, y_actual=None):\n",
    "        ml_model = self.LinearRegressor_ML(X_train, y_train)\n",
    "        return self.prediction(ml_model, X_test, y_actual)\n",
    "\n",
    "    def LinearRegressor_ML(self, X_train, y_train):\n",
    "        # print(\"*** Random LinearRegression Regression ***\")\n",
    "        ML_Alg = LinearRegression()\n",
    "        ml_model = ML_Alg.fit(X_train, np.ravel(y_train))\n",
    "        return ml_model\n",
    "\n",
    "\n",
    "    def GradientBoostingRegressor_run(self, X_train, y_train, X_test, y_actual=None):\n",
    "        ml_model = self.GradientBoostingRegressor_ML(X_train, y_train)\n",
    "        return self.prediction(ml_model, X_test, y_actual)\n",
    "\n",
    "    def GradientBoostingRegressor_ML(self, X_train, y_train):\n",
    "        # print(\"*** Gradient Boosting Regression ***\")\n",
    "        ML_Alg = GradientBoostingRegressor(n_estimators=1000,\n",
    "                                           learning_rate=0.1,\n",
    "                                           subsample=0.5,\n",
    "                                           max_depth=1,\n",
    "                                           random_state=0)\n",
    "        ml_model = ML_Alg.fit(X_train, np.ravel(y_train))\n",
    "        return ml_model\n",
    "\n",
    "    def GaussianProcessRegressor_run(self, X_train, y_train, X_test, y_actual=None):\n",
    "        ml_model = self.GaussianProcessRegressor_ML(X_train, y_train)\n",
    "        return self.prediction(ml_model, X_test, y_actual)\n",
    "\n",
    "    def GaussianProcessRegressor_ML(self, X_train, y_train):\n",
    "        # print(\"*** Gaussian Process Regression ***\")\n",
    "        kernel = DotProduct() + WhiteKernel()\n",
    "        ML_Alg = GaussianProcessRegressor(kernel=kernel, random_state=0)\n",
    "        ml_model = ML_Alg.fit(X_train, np.ravel(y_train))\n",
    "        return ml_model\n",
    "\n",
    "    def prediction(self, ml_model, X_test, y_actual=None):\n",
    "        y_predicted = ml_model.predict(X_test)\n",
    "        return y_predicted, self.show_results(y_predicted, y_actual)\n",
    "\n",
    "    def run_with_evidence_and_check_prediction(self, dmp, model, data, y_actual):\n",
    "        predicted = []\n",
    "        i = 0\n",
    "        for index, rows in data.iterrows():\n",
    "            keys = rows.keys()\n",
    "            sbn = model\n",
    "            y_name = y_actual.columns[0]\n",
    "            y_value = y_actual.iloc[i, 0]\n",
    "            for k in keys:\n",
    "                sbn += \"defineEvidence({}, {});\".format(k, rows.get(k))\n",
    "            sbn += \"run(DMP);\"\n",
    "            dmp.run(sbn)\n",
    "\n",
    "            # check prediction\n",
    "            # print(\"================ Prediction results from BN ================\")\n",
    "            # print(dmp.output)\n",
    "            with open(dmp.output) as json_file:\n",
    "                net = json.load(json_file)\n",
    "                for node in net:\n",
    "                    for obj, contents in node.items():\n",
    "                        # print('node: ' + obj)\n",
    "                        mean = None\n",
    "                        for attr, values in contents.items():\n",
    "                            # print(\" \" + attr + ': ' + str(values))\n",
    "                            if attr == \"marginal\":\n",
    "                                mean = values[\"MU\"]\n",
    "\n",
    "                        if obj == y_name:\n",
    "                            # print(\"================================\")\n",
    "                            # print(\"{} : Predicted {} : Actual {}\".format(y_name, mean, y_value))\n",
    "                            predicted.append(float(mean))\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return predicted\n",
    "\n",
    "    def ContinuousBNRegressor_run(self, name, X_train, y_train, X_test, y_actual=None, show=True):\n",
    "        ssbn = self.ContinuousBNRegressor_ML(name, X_train, y_train)\n",
    "        return self.ContinuousBNRegressor_prediction(name, ssbn, X_test, y_actual, show)\n",
    "\n",
    "    def ContinuousBNRegressor_ML(self, name, X_train, y_train):\n",
    "        # print(\"*** Continuous BN Regressor  ***\")\n",
    "        #############################\n",
    "        # Make a csv data file\n",
    "        csv = r'../TestData/{}_for_test.csv'.format(name)\n",
    "        # csv = \"E:/SW-Posco2019/DATAETL/TestData/big_data_1000000.csv\"\n",
    "        output = r'../Output_BN/{}_ssbn.txt'.format(name)\n",
    "\n",
    "        df_col = pd.concat([X_train, y_train], axis=1)\n",
    "        df_col.to_csv(csv, index=None)\n",
    "\n",
    "        #############################\n",
    "        # Run MEBN learning\n",
    "        ts = time.time()\n",
    "        hml = HML_runner()\n",
    "\n",
    "        # Make a V-BN model\n",
    "        parents = []\n",
    "        child = y_train.columns[0]\n",
    "        for nodeName in X_train.columns:\n",
    "            parents.append(nodeName)\n",
    "\n",
    "        hml.make_Model(child, parents)\n",
    "\n",
    "        # Run MEBN learning\n",
    "        ssbn = hml.run(csv, output)\n",
    "\n",
    "        # print(\"== Mahcine learning end: Time {} ==============================\".format(time.time()-ts))\n",
    "        return ssbn\n",
    "\n",
    "    def ContinuousBNRegressor_prediction(self, name, ssbn, X_test, y_actual=None, show=True):\n",
    "        #############################\n",
    "        # Prediction\n",
    "        ts = time.time()\n",
    "        output = r\"../Output_BN/{}_bn_output.json\".format(name)\n",
    "        dmp = DMP_runner(output)\n",
    "\n",
    "        y_predicted = self.run_with_evidence_and_check_prediction(dmp, ssbn, X_test, y_actual)\n",
    "\n",
    "        # print(\"== BN was completed : Time {} ==============================\".format(time.time()-ts))\n",
    "\n",
    "        if show == True and len(y_actual) > 1:\n",
    "            return y_predicted, self.show_results(y_predicted, y_actual)\n",
    "        else:\n",
    "            return y_predicted, self.show_results(y_predicted, y_actual=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import threading\n",
    "from statistics import mean\n",
    "import math\n",
    "import warnings\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from datetime import datetime\n",
    "\n",
    "class MLModelFamily:\n",
    "    \"\"\"\n",
    "    This class is used as a structure of machine learning family\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, id=0):\n",
    "        self.id = id\n",
    "        self.models = {}\n",
    "        self.models = {}\n",
    "        self.data = {}\n",
    "\n",
    "    def add_CL(self, cl_alg):\n",
    "        self.models[cl_alg] = {}\n",
    "        self.data[cl_alg] = {}\n",
    "\n",
    "    def get_CL(self):\n",
    "        return self.models\n",
    "\n",
    "    def del_CL(self, cl_alg):\n",
    "        del self.models[cl_alg]\n",
    "        del self.data[cl_alg]\n",
    "\n",
    "    def add_CL_param(self, cl_alg, parameters):\n",
    "        self.models[cl_alg][parameters] = {}\n",
    "        self.data[cl_alg][parameters] = {}\n",
    "\n",
    "    def del_CL_param(self, cl_alg, parameters):\n",
    "        del self.models[cl_alg][parameters]\n",
    "        del self.data[cl_alg][parameters]\n",
    "\n",
    "    def add_CL_model(self, cl_alg, parameters, cl):\n",
    "        if 'CL_MODEL' not in self.models[cl_alg][parameters]:\n",
    "            self.models[cl_alg][parameters]['CL_MODEL'] = {}\n",
    "\n",
    "        self.models[cl_alg][parameters]['CL_MODEL'] = cl\n",
    "\n",
    "    def set_CL_model_avg_r2(self, cl_alg, parameters, avg_r2):\n",
    "        self.models[cl_alg][parameters]['AVG_R2'] = avg_r2\n",
    "\n",
    "    def get_CL_model_avg_r2(self, cl_alg, parameters):\n",
    "        return self.models[cl_alg][parameters]['AVG_R2']\n",
    "\n",
    "    def set_CL_total_avg_r2(self, cl_alg, avg_r2):\n",
    "        self.models[cl_alg]['TOTAL_AVG_R2'] = avg_r2\n",
    "\n",
    "    def get_CL_total_avg_r2(self, cl_alg):\n",
    "        return self.models[cl_alg]['TOTAL_AVG_R2']\n",
    "\n",
    "    def set_CL_data(self, cl_alg, parameters, cur_cluster, data_name, data):\n",
    "        if cur_cluster not in self.data[cl_alg][parameters]:\n",
    "            self.data[cl_alg][parameters][cur_cluster] = {}\n",
    "        self.data[cl_alg][parameters][cur_cluster][data_name] = data\n",
    "\n",
    "    def add_SL(self, cl_alg, parameters, cur_cluster, prediction_alg):\n",
    "        if cur_cluster not in self.models[cl_alg][parameters]:\n",
    "            self.models[cl_alg][parameters][cur_cluster] = {}\n",
    "\n",
    "        self.models[cl_alg][parameters][cur_cluster][prediction_alg] = {}\n",
    "\n",
    "    def del_SL(self, cl_alg, parameters, cur_cluster, prediction_alg):\n",
    "        del self.models[cl_alg][parameters][cur_cluster][prediction_alg]\n",
    "\n",
    "    def add_SL_model(self, cl_alg, parameters, cur_cluster, prediction_alg, model):\n",
    "        self.models[cl_alg][parameters][cur_cluster][prediction_alg]['SL_MODEL'] = model\n",
    "\n",
    "    def set_SL_model_r2(self, cl_alg, parameters, cur_cluster, prediction_alg, r2):\n",
    "        if math.isnan(r2):\n",
    "            # In some cases, the training data has the size of 1, then R2 becomes NaN.\n",
    "            warnings.warn(f'{cl_alg}.{parameters}.{cur_cluster}.{prediction_alg}: The prediction result was NaN.')\n",
    "            self.models[cl_alg][parameters][cur_cluster][prediction_alg]['R2'] = -10\n",
    "        else:\n",
    "            self.models[cl_alg][parameters][cur_cluster][prediction_alg]['R2'] = r2\n",
    "\n",
    "    def get_SL_model_r2(self, cl_alg, parameters, cur_cluster, prediction_alg):\n",
    "        return self.models[cl_alg][parameters][cur_cluster][prediction_alg]['R2']\n",
    "\n",
    "    def get_sl_models(self):\n",
    "        \"\"\"\n",
    "        This returns all selected supervised learning models in the ML model family\n",
    "        :return: a dictionary for pairs of supervised learning models\n",
    "        e.g., ) {0: [SL model object], 1: [SL model object], ...}\n",
    "        0 and 1 stand for the cluster id\n",
    "        \"\"\"\n",
    "\n",
    "        cl = next(iter(self.models.values()))\n",
    "        cl_model = next(iter(cl.values()))\n",
    "        result = {}\n",
    "\n",
    "        for cluster_id, value in cl_model.items():\n",
    "            if cluster_id is not 'CL_MODEL' and cluster_id is not 'AVG_R2':\n",
    "                sl_model = next(iter(value.values()))\n",
    "                result[cluster_id] = sl_model\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_sl_model(self, cluster_id):\n",
    "        \"\"\"\n",
    "        This returns a selected supervised learning model associated with a cluster id\n",
    "        :param cluster_id: the index of a cluster in a clustering model in the ML model family\n",
    "        :return: a SL model name, a SL model object\n",
    "        \"\"\"\n",
    "\n",
    "        cl = next(iter(self.models.values()))\n",
    "        cl_model = next(iter(cl.values()))\n",
    "\n",
    "        for key, value in cl_model.items():\n",
    "            if key is not 'CL_MODEL' and key is not 'AVG_R2':\n",
    "                if key == cluster_id:\n",
    "                    sl_model = next(iter(value.values()))\n",
    "                    return list(value.keys())[0], sl_model['SL_MODEL']\n",
    "\n",
    "        return None, None\n",
    "\n",
    "    def get_cl_model(self):\n",
    "        \"\"\"\n",
    "        This returns the clustering algorithm class\n",
    "        \"\"\"\n",
    "\n",
    "        cl = next(iter(self.models.values()))\n",
    "        cl_model = next(iter(cl.values()))\n",
    "        if isinstance(cl_model['CL_MODEL'], Clustering_Alg):\n",
    "            return cl_model['CL_MODEL']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_clustering_alg(self):\n",
    "        \"\"\"\n",
    "        This returns a selected clustering algorithm\n",
    "        \"\"\"\n",
    "\n",
    "        cl_model = self.get_cl_model()\n",
    "        if cl_model is not None:\n",
    "            return cl_model.get_selected_clustering_alg()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_CL_models(self, cl_alg, parameters):\n",
    "        cl_models = {}\n",
    "        for key, value in self.models[cl_alg][parameters].items():\n",
    "            if key is not 'CL_MODEL' and key is not 'AVG_R2':\n",
    "                cl_models[key] = value\n",
    "\n",
    "        return cl_models\n",
    "\n",
    "    def set_high_CL_model(self, cl_alg, alg_high):\n",
    "        self.models[cl_alg]['HIGH_CL'] = alg_high\n",
    "\n",
    "    def get_high_CL_model(self, cl_alg):\n",
    "        return self.models[cl_alg]['HIGH_CL']\n",
    "\n",
    "    def get_clustered_data_by_id(self, cluster_id):\n",
    "        \"\"\"\n",
    "        This returns a clustered data associated with a cluster id\n",
    "        :param cluster_id: the index of a cluster in a clustering model in the ML model family\n",
    "        :return: X data, y data\n",
    "        \"\"\"\n",
    "\n",
    "        cl = next(iter(self.data.values()))\n",
    "        cl_data= next(iter(cl.values()))\n",
    "\n",
    "        X, y = None, None\n",
    "        for key, data_dict in cl_data.items():\n",
    "            if key == cluster_id:\n",
    "                X = data_dict['x_train']\n",
    "                y = data_dict['y_train']\n",
    "                break\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def get_clustered_all_data_by_id(self, cluster_id):\n",
    "        X, y = self.get_clustered_data_by_id(cluster_id)\n",
    "        data = pd.concat((X, y), axis=1)\n",
    "        return data\n",
    "\n",
    "class DataClusterBasedMachineLearning:\n",
    "    \"\"\"\n",
    "    Data Cluster based Machine Learning (DC-ML)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_x, data_y, clustering_variables, clustering_algs, prediction_algs, metrics='MAE'):\n",
    "        \"\"\"\n",
    "        :param data: A training data set D\n",
    "        :param clustering_alg: A set of clustering algorithms C\n",
    "        :param prediction_alg: A set of prediction algorithms P\n",
    "        :param max_clusters: A maximum number of clusters m\n",
    "        \"\"\"\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        self.clustering_variables = clustering_variables\n",
    "        self.clustering_algs = clustering_algs\n",
    "        self.prediction_algs = prediction_algs\n",
    "\n",
    "        # Initialize an ML Model Family\n",
    "        # It contains all ML models (i.e., Clustering Models (CL) and their Supervised Learning (SL) Models)\n",
    "        self.ml_family = MLModelFamily()\n",
    "\n",
    "        # For experiment\n",
    "        self.is_experiment = True\n",
    "        self.set_metrics(metrics)\n",
    "\n",
    "    def set_metrics(self, metrics):\n",
    "        # Set metrics\n",
    "        self.metrics = metrics\n",
    "        if metrics == 'R2':\n",
    "            self.score = r2_score\n",
    "        elif metrics == 'MAE':\n",
    "            self.score = mean_absolute_error\n",
    "\n",
    "    def min_or_max(self):\n",
    "        if self.metrics == 'R2':\n",
    "            return max, float('-inf')\n",
    "        elif self.metrics == 'MAE':\n",
    "            return min, float('inf')\n",
    "\n",
    "    def do_machine_learning(self, prediction_alg, x_train, y_train, cbn_name='temp'):\n",
    "        \"\"\"\n",
    "        This performs common SL learning\n",
    "        :param prediction_alg: A name of SL learning\n",
    "        :param x_train: Training data for X variables\n",
    "        :param y_train: Training data for a y variable\n",
    "        :param cbn_name: A special parameter for continuous BN learning\n",
    "        :return: A SL learning object\n",
    "        \"\"\"\n",
    "\n",
    "        if prediction_alg is 'GradientBoostingRegressor':\n",
    "            model = RegressionML(self.metrics).GradientBoostingRegressor_ML(x_train, y_train)\n",
    "        elif prediction_alg is 'RandomForestRegressor':\n",
    "            model = RegressionML(self.metrics).RandomForestRegressor_ML(x_train, y_train)\n",
    "        elif prediction_alg is 'GaussianProcessRegressor':\n",
    "            model = RegressionML(self.metrics).GaussianProcessRegressor_ML(x_train, y_train)\n",
    "        elif prediction_alg is 'LinearRegression':\n",
    "            model = RegressionML(self.metrics).LinearRegressor_ML(x_train, y_train)\n",
    "        elif prediction_alg is 'ContinuousBNRegressor':\n",
    "            model = RegressionML(self.metrics).ContinuousBNRegressor_ML(cbn_name, x_train, y_train)\n",
    "        return model\n",
    "\n",
    "    def do_prediction(self, model_name, model, X, y=None, cbn_name='temp'):\n",
    "        \"\"\"\n",
    "        This performs prediction using a given SL learning\n",
    "        :param model_name: A name of SL learning\n",
    "        :param model: An object of SL learning\n",
    "        :param X: Data for X variables\n",
    "        :param y: Data for a y variable\n",
    "        :param cbn_name: A special parameter for continuous BN learning\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if model_name is 'ContinuousBNRegressor':\n",
    "            yPredicted, r2 = RegressionML(self.metrics).ContinuousBNRegressor_prediction(cbn_name, model, X, y)\n",
    "        else:\n",
    "            yPredicted, r2 = RegressionML(self.metrics).prediction(model, X, y)\n",
    "        return yPredicted, r2\n",
    "\n",
    "    def perform_machine_learning_alg(self, cl_alg, parameters, cur_cluster, prediction_alg, x_clustered, y_clustered):\n",
    "        \"\"\"\n",
    "        This performs an SL algorithm\n",
    "        :param cl_alg: The name of a clustering algorithm\n",
    "        :param parameters:\n",
    "        :param cur_cluster: The current cluster index of the clustering algorithm\n",
    "        :param prediction_alg: A name of SL learning\n",
    "        :param x_train: Training data for X variables\n",
    "        :param x_val: Validation data for X variables\n",
    "        :param y_train: Training data for a y variable\n",
    "        :param y_val: Validation data for a y variable\n",
    "        \"\"\"\n",
    "\n",
    "        # temporary name for CBN\n",
    "        cbn_name = f'{cl_alg}_{parameters}_{cur_cluster}_{datetime.now().strftime(\"%m_%d_%Y %H_%M_%S\")}'\n",
    "        n_splits = 2\n",
    "        kf = KFold(n_splits=n_splits)\n",
    "\n",
    "        r2_avg = 0\n",
    "        for train_index, val_index in kf.split(x_clustered):\n",
    "            x_train, x_val = x_clustered.iloc[train_index], x_clustered.iloc[val_index]\n",
    "            y_train, y_val = y_clustered.iloc[train_index], y_clustered.iloc[val_index]\n",
    "\n",
    "            # perform ML\n",
    "            model = self.do_machine_learning(prediction_alg, x_train, y_train, cbn_name=cbn_name)\n",
    "\n",
    "            # perform prediction\n",
    "            yPredicted, r2 = self.do_prediction(prediction_alg, model, x_val, y_val, cbn_name=cbn_name)\n",
    "            r2_avg += r2\n",
    "\n",
    "        r2_avg = r2_avg/n_splits\n",
    "        # print(cbn_name, prediction_alg, model.feature_importances_)\n",
    "\n",
    "        self.ml_family.add_SL_model(cl_alg, parameters, cur_cluster, prediction_alg, model)\n",
    "        self.ml_family.set_SL_model_r2(cl_alg, parameters, cur_cluster, prediction_alg, r2_avg)\n",
    "\n",
    "    def perform_machine_learning(self, cl_alg, parameters, cur_cluster, x_clustered, y_clustered):\n",
    "        \"\"\"\n",
    "        This executes a set of SL algorithms\n",
    "        :param cl_alg: The name of a clustering algorithm\n",
    "        :param parameters:\n",
    "        :param cur_cluster: The current cluster index of the clustering algorithm\n",
    "        :param x_train: Training data for X variables\n",
    "        :param x_val: Validation data for X variables\n",
    "        :param y_train: Training data for a y variable\n",
    "        :param y_val: Validation data for a y variable\n",
    "        \"\"\"\n",
    "\n",
    "        #############################################################################\n",
    "        # 1. perform common machine learning\n",
    "        thread = []\n",
    "        for prediction_alg in self.prediction_algs:\n",
    "            self.ml_family.add_SL(cl_alg, parameters, cur_cluster, prediction_alg)\n",
    "\n",
    "            print(f'[Thread] {cl_alg}.{parameters}.{cur_cluster} -> ML alg {prediction_alg}')\n",
    "\n",
    "            t = threading.Thread(target=self.perform_machine_learning_alg, args=([cl_alg, parameters, cur_cluster, prediction_alg, x_clustered, y_clustered]))\n",
    "            t.setDaemon(True)\n",
    "            thread.append(t)\n",
    "\n",
    "        for t in thread:\n",
    "            t.start()\n",
    "\n",
    "        for t in thread:\n",
    "            t.join()\n",
    "\n",
    "        #############################################################################\n",
    "        # 2.Select a best prediction alg and remove all low-scored algorithms\n",
    "        alg_high = None\n",
    "        min_or_max, r2_high = self.min_or_max()\n",
    "        for prediction_alg in self.prediction_algs:\n",
    "            r2 = self.ml_family.get_SL_model_r2(cl_alg, parameters, cur_cluster, prediction_alg)\n",
    "\n",
    "            print(f'Check R2 for {cl_alg}.{parameters}.{cur_cluster}.{prediction_alg} = {r2}')\n",
    "            r2_high = min_or_max(r2_high, r2)\n",
    "            if r2 is r2_high:\n",
    "                if alg_high is not None:\n",
    "                    self.ml_family.del_SL(cl_alg, parameters, cur_cluster, alg_high)\n",
    "                alg_high = prediction_alg\n",
    "            else:\n",
    "                self.ml_family.del_SL(cl_alg, parameters, cur_cluster, prediction_alg)\n",
    "\n",
    "    def perform_clustering_alg_with_clusters(self, cl_alg, parameters):\n",
    "        \"\"\"\n",
    "        This performs a clustering algorithm using a given maximum number of clusters\n",
    "        :param cl_alg: A clustering algorithm\n",
    "        :param parameters:\n",
    "        \"\"\"\n",
    "\n",
    "        #############################################################################\n",
    "        # 1. Perform clustering algorithm using input training data\n",
    "        print(f'perform_prediction_alg {cl_alg} with the cluster {parameters}')\n",
    "        cl = Clustering_Alg()\n",
    "        cl.set_algs(cl_alg)\n",
    "        cl.set_base(parameters)\n",
    "        cl.set_data(self.data_x, self.data_y, self.clustering_variables)\n",
    "        cl.run()\n",
    "\n",
    "        # Note that the clustering algorithm can change the number of clusters\n",
    "        # e.g., ) The default n_clusters = 3 changes to n_clusters = 2 according to the clustering result\n",
    "        self.ml_family.add_CL_model(cl_alg, parameters, cl)\n",
    "\n",
    "        # Get clustered data from the cluster model\n",
    "        data, data_x, data_y = cl.get_clustered_data_XY(cl_alg)\n",
    "\n",
    "        #############################################################################\n",
    "        # 2. Check cluster consistency:\n",
    "        # If a cluster contains only one datum, data grouped by the cluster is determined as inconsistent data\n",
    "        # Then, return it with a lowest score\n",
    "        for cur_cluster, datum in data_x.items():\n",
    "            if len(data_y[cur_cluster]) == 1:\n",
    "                print(len(data_y[cur_cluster]))\n",
    "            print(f'data split for {cl_alg}.{parameters}.{cur_cluster} X-Size[{len(data_x[cur_cluster])}] Y-Size[{len(data_y[cur_cluster])}]')\n",
    "\n",
    "            # inconsistent clustered data!\n",
    "            if len(data_x[cur_cluster]) != len(data_y[cur_cluster]):\n",
    "                warnings.warn('inconsistent clustered data!')\n",
    "                self.ml_family.set_CL_model_avg_r2(cl_alg, parameters, -10000)\n",
    "                return\n",
    "\n",
    "        #############################################################################\n",
    "        # 3. Preparing SL learning\n",
    "        thread = []\n",
    "\n",
    "        for cur_cluster, datum in data_x.items():\n",
    "            # split data for machine learning\n",
    "            # x_train, x_val, y_train, y_val = train_test_split(data_x[cur_cluster], data_y[cur_cluster], test_size=test_size)\n",
    "\n",
    "            if self.is_experiment:\n",
    "                # print(f'sub-training data size[{len(y_train)}], validation data size[{len(y_val)}]')\n",
    "\n",
    "                self.ml_family.set_CL_data(cl_alg, parameters, cur_cluster, 'x_train', data_x[cur_cluster])\n",
    "                self.ml_family.set_CL_data(cl_alg, parameters, cur_cluster, 'y_train', data_y[cur_cluster])\n",
    "\n",
    "            t = threading.Thread(target=self.perform_machine_learning, args=([cl_alg, parameters, cur_cluster, data_x[cur_cluster], data_y[cur_cluster]]))\n",
    "            t.setDaemon(True)\n",
    "            thread.append(t)\n",
    "\n",
    "        for t in thread:\n",
    "            t.start()\n",
    "\n",
    "        for t in thread:\n",
    "            t.join()\n",
    "\n",
    "        #############################################################################\n",
    "        # 4. Calculate the average R2 and store it to 'ml_models.cl_alg.parameters.avg_r2'\n",
    "        avg_r2 = []\n",
    "        cl_models = self.ml_family.get_CL_models(cl_alg, parameters)\n",
    "\n",
    "        for cur_cluster, ml_alg_r2 in cl_models.items():\n",
    "            try:\n",
    "                r2 = list(ml_alg_r2.values())[0]['R2']\n",
    "            except IndexError:\n",
    "                print('list index out of range')\n",
    "            avg_r2.append(r2)\n",
    "\n",
    "        mean_avg_r2 = mean(avg_r2)\n",
    "        self.ml_family.set_CL_model_avg_r2(cl_alg, parameters, mean_avg_r2)\n",
    "\n",
    "    def perform_clustering(self, cl_alg):\n",
    "        \"\"\"\n",
    "        This performs clustering\n",
    "        :param cl_alg: A clustering algorithm\n",
    "        \"\"\"\n",
    "\n",
    "        #############################################################################\n",
    "        # 1. Perform clustering\n",
    "        thread = []\n",
    "        # for parameters in range(2, self.max_clusters + 1):\n",
    "        for parameters in self.clustering_algs[cl_alg]:\n",
    "            self.ml_family.add_CL_param(cl_alg, parameters)\n",
    "            print(f'[Thread] clustering alg {cl_alg} with the cluster parameter {parameters} start')\n",
    "            t = threading.Thread(target=self.perform_clustering_alg_with_clusters, args=([cl_alg, parameters]))\n",
    "            t.setDaemon(True)\n",
    "            thread.append(t)\n",
    "\n",
    "        for t in thread:\n",
    "            t.start()\n",
    "\n",
    "        for t in thread:\n",
    "            t.join()\n",
    "\n",
    "        #############################################################################\n",
    "        # 2. select a best number of clusters and remove all low-scored models\n",
    "        cl_num_high = None\n",
    "        min_or_max, avg_r2_high = self.min_or_max()\n",
    "\n",
    "        # for parameters in range(2, self.max_clusters + 1):\n",
    "        for parameters in self.clustering_algs[cl_alg]:\n",
    "            avg_r2 = self.ml_family.get_CL_model_avg_r2(cl_alg, parameters)\n",
    "\n",
    "            print(f'Check avg_r2 for {cl_alg}.{parameters} = {avg_r2}')\n",
    "            avg_r2_high = min_or_max(avg_r2_high, avg_r2)\n",
    "            if avg_r2 is avg_r2_high:\n",
    "                if cl_num_high is not None:\n",
    "                    self.ml_family.del_CL_param(cl_alg, cl_num_high)\n",
    "                cl_num_high = parameters\n",
    "            else:\n",
    "                self.ml_family.del_CL_param(cl_alg, parameters)\n",
    "\n",
    "        self.ml_family.set_CL_total_avg_r2(cl_alg, avg_r2_high)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        This is a main function for running DC-ML learning\n",
    "        :return: An ML model family object\n",
    "        \"\"\"\n",
    "\n",
    "        #############################################################################\n",
    "        # 1. Perform clustering\n",
    "        thread = []\n",
    "        for cl_alg in self.clustering_algs:\n",
    "            self.ml_family.add_CL(cl_alg)\n",
    "            print(f'[Thread] {cl_alg} start')\n",
    "            t = threading.Thread(target=self.perform_clustering, args=([cl_alg]))\n",
    "            t.setDaemon(True)\n",
    "            thread.append(t)\n",
    "\n",
    "        for t in thread:\n",
    "            t.start()\n",
    "\n",
    "        for t in thread:\n",
    "            t.join()\n",
    "\n",
    "        #############################################################################\n",
    "        # 2. perform ML for non-cluster models\n",
    "        # 'NON-CLUSTER' is used for machine learning of SL models without clustering\n",
    "        # self.ml_family.add_CL('NON-CLUSTER')\n",
    "        # self.ml_family.add_CL_param('NON-CLUSTER', 1)\n",
    "        # self.ml_family.add_CL_model('NON-CLUSTER', 1, 0)\n",
    "        # self.ml_family.set_CL_data('NON-CLUSTER', 1, 0, 'x_train', self.data_x)\n",
    "        # self.ml_family.set_CL_data('NON-CLUSTER', 1, 0, 'y_train', self.data_y)\n",
    "        # self.perform_machine_learning('NON-CLUSTER', 1, 0, self.data_x, self.data_y)\n",
    "        # cl_models = self.ml_family.get_CL_models('NON-CLUSTER', 1)\n",
    "        # for cur_cluster, ml_alg_r2 in cl_models.items():\n",
    "        #     r2 = list(ml_alg_r2.values())[0]['R2']\n",
    "        # self.ml_family.set_CL_total_avg_r2('NON-CLUSTER', r2)\n",
    "\n",
    "        #############################################################################\n",
    "        # 3. select a high-scored model\n",
    "        min_or_max, avg_r2_high = self.min_or_max()\n",
    "        high_scored_cl = None\n",
    "        cl_algs = list(self.ml_family.get_CL().keys())\n",
    "        for cl_alg in cl_algs:\n",
    "            avg_r2 = self.ml_family.get_CL_total_avg_r2(cl_alg)\n",
    "            avg_r2_high = min_or_max(avg_r2_high, avg_r2)\n",
    "            if avg_r2 is avg_r2_high:\n",
    "                if high_scored_cl is not None:\n",
    "                    self.ml_family.del_CL(high_scored_cl)\n",
    "\n",
    "                high_scored_cl = cl_alg\n",
    "            else:\n",
    "                self.ml_family.del_CL(cl_alg)\n",
    "\n",
    "        #############################################################################\n",
    "        # 4. perform ML **again** using all data of both training and validation data sets\n",
    "        sl_models = self.ml_family.get_sl_models()\n",
    "        for cluster_id, sl in sl_models.items():\n",
    "            # print(cluster_id, sl)\n",
    "            x_train, y_train = self.ml_family.get_clustered_data_by_id(cluster_id)\n",
    "            # self.data[cl_alg][parameters]\n",
    "\n",
    "            # temporary name for CBN\n",
    "            cbn_name = f'{cl_alg}_{cluster_id}_{datetime.now().strftime(\"%m_%d_%Y %H_%M_%S\")}'\n",
    "\n",
    "            if isinstance(sl['SL_MODEL'], str):\n",
    "                sl_name = 'ContinuousBNRegressor'\n",
    "            else:\n",
    "                sl_name = type(sl['SL_MODEL']).__name__\n",
    "\n",
    "            # perform ML\n",
    "            model = self.do_machine_learning(sl_name, x_train, y_train, cbn_name=cbn_name)\n",
    "\n",
    "            # Replace the old with the new SL model\n",
    "            sl['SL_MODEL'] = model\n",
    "\n",
    "        print('!!! An ML model family was selected !!!')\n",
    "\n",
    "        return self.ml_family\n",
    "\n",
    "    def perform_prediction(self, x_test, y_test, metrics='R2'):\n",
    "        \"\"\"\n",
    "        This performs prediction given a test data set\n",
    "        :param x_test: test data for X variables\n",
    "        :param y_test: test data for a y variable\n",
    "        :return: predicted y values, R2 scores\n",
    "        \"\"\"\n",
    "\n",
    "        self.set_metrics(metrics)\n",
    "\n",
    "        cl_alg = self.ml_family.get_clustering_alg()\n",
    "        y_label = []\n",
    "\n",
    "        # None-cl_model means that the Non-Cluster model was selected\n",
    "        if cl_alg is not None:\n",
    "            # Get data using clustering variables\n",
    "            data_for_clustering = x_test[self.clustering_variables]\n",
    "\n",
    "            cl_model = self.ml_family.get_cl_model()\n",
    "            # normalize dataset for easier parameter selection\n",
    "            x_test_norm = cl_model.data_scaler(data_for_clustering)\n",
    "            # predict label using the normalized data\n",
    "            y_label = cl_alg.predict(x_test_norm)\n",
    "\n",
    "            index = 0\n",
    "            yPredicted = []\n",
    "\n",
    "            for cluster_id in y_label:\n",
    "                ml_name, ml_model = self.ml_family.get_sl_model(cluster_id)\n",
    "                yPre, r2 = self.do_prediction(ml_name, ml_model, x_test.iloc[[index]], y_test.iloc[[index]], cbn_name = datetime.now().strftime(\"%m_%d_%Y %H_%M_%S\"))\n",
    "                yPredicted.append(yPre)\n",
    "                index += 1\n",
    "\n",
    "            yPredicted2 = pd.DataFrame(yPredicted)\n",
    "            r2 = self.score(y_test, yPredicted2)\n",
    "        else:\n",
    "            ml_name, ml_model = self.ml_family.get_sl_model(0)\n",
    "            yPredicted, r2 = self.do_prediction(ml_name, ml_model, x_test, y_test, cbn_name=datetime.now().strftime(\"%m_%d_%Y %H_%M_%S\"))\n",
    "\n",
    "        return yPredicted, r2, y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Thread] GaussianMixture start\n",
      "[Thread] Birch start\n",
      "[Thread] MiniBatchKMeans start\n",
      "[Thread] clustering alg GaussianMixture with the cluster parameter 2 start\n",
      "[Thread] clustering alg GaussianMixture with the cluster parameter 3 start\n",
      "[Thread] clustering alg GaussianMixture with the cluster parameter 4 start\n",
      "[Thread] clustering alg Birch with the cluster parameter 2 startperform_prediction_alg GaussianMixture with the cluster 2\n",
      "[Thread] clustering alg MiniBatchKMeans with the cluster parameter 2 start\n",
      "[Thread] clustering alg MiniBatchKMeans with the cluster parameter 3 start\n",
      "[Thread] clustering alg MiniBatchKMeans with the cluster parameter 4 start\n",
      "[Thread] clustering alg Birch with the cluster parameter 3 start\n",
      "perform_prediction_alg MiniBatchKMeans with the cluster 2\n",
      "mean[691.02916667], var[639.89289931], n_samples[24], scale[[25.29610443]][Thread] clustering alg Birch with the cluster parameter 4 start\n",
      "perform_prediction_alg GaussianMixture with the cluster 3\n",
      "\n",
      "perform_prediction_alg MiniBatchKMeans with the cluster 3\n",
      "\n",
      "mean[691.02916667], var[639.89289931], n_samples[24], scale[[25.29610443]]mean[691.02916667], var[639.89289931], n_samples[24], scale[[25.29610443]]\n",
      "perform_prediction_alg Birch with the cluster 2\n",
      "perform_prediction_alg GaussianMixture with the cluster 4\n",
      "mean[691.02916667], var[639.89289931], n_samples[24], scale[[25.29610443]]\n",
      "\n",
      "perform_prediction_alg Birch with the cluster 3\n",
      "mean[691.02916667], var[639.89289931], n_samples[24], scale[[25.29610443]]\n",
      "perform_prediction_alg MiniBatchKMeans with the cluster 4\n",
      "mean[691.02916667], var[639.89289931], n_samples[24], scale[[25.29610443]]perform_prediction_alg Birch with the cluster 4\n",
      "mean[691.02916667], var[639.89289931], n_samples[24], scale[[25.29610443]]mean[691.02916667], var[639.89289931], n_samples[24], scale[[25.29610443]]\n",
      "\n",
      "\n",
      "mean[691.02916667], var[639.89289931], n_samples[24], scale[[25.29610443]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_birch.py:638: ConvergenceWarning: Number of subclusters found (3) by Birch is less than (4). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data split for Birch.2.1 X-Size[9] Y-Size[9]data split for Birch.4.0 X-Size[9] Y-Size[9]data split for GaussianMixture.2.1 X-Size[9] Y-Size[9]\n",
      "data split for GaussianMixture.2.0 X-Size[15] Y-Size[15]\n",
      "[Thread] GaussianMixture.2.1 -> ML alg GaussianProcessRegressor\n",
      "[Thread] GaussianMixture.2.1 -> ML alg RandomForestRegressor\n",
      "\n",
      "\n",
      "data split for Birch.2.0 X-Size[15] Y-Size[15]\n",
      "[Thread] GaussianMixture.2.0 -> ML alg GaussianProcessRegressor\n",
      "[Thread] GaussianMixture.2.0 -> ML alg RandomForestRegressor\n",
      "[Thread] Birch.2.1 -> ML alg GaussianProcessRegressor\n",
      "[Thread] Birch.2.1 -> ML alg RandomForestRegressor\n",
      "data split for GaussianMixture.4.1 X-Size[4] Y-Size[4]\n",
      "data split for GaussianMixture.4.2 X-Size[10] Y-Size[10]\n",
      "data split for GaussianMixture.4.0 X-Size[5] Y-Size[5]\n",
      "data split for GaussianMixture.4.3 X-Size[5] Y-Size[5]\n",
      "[Thread] GaussianMixture.4.1 -> ML alg GaussianProcessRegressor\n",
      "[Thread] GaussianMixture.4.1 -> ML alg RandomForestRegressor\n",
      "data split for GaussianMixture.3.1 X-Size[9] Y-Size[9][Thread] Birch.2.0 -> ML alg GaussianProcessRegressor\n",
      "[Thread] Birch.2.0 -> ML alg RandomForestRegressor\n",
      "\n",
      "data split for Birch.4.1 X-Size[10] Y-Size[10]\n",
      "data split for Birch.4.2 X-Size[5] Y-Size[5]data split for GaussianMixture.3.2 X-Size[10] Y-Size[10]\n",
      "data split for GaussianMixture.3.0 X-Size[5] Y-Size[5]\n",
      "[Thread] GaussianMixture.3.1 -> ML alg GaussianProcessRegressor\n",
      "[Thread] GaussianMixture.3.1 -> ML alg RandomForestRegressor\n",
      "[Thread] GaussianMixture.4.2 -> ML alg GaussianProcessRegressor\n",
      "[Thread] GaussianMixture.4.2 -> ML alg RandomForestRegressor\n",
      "data split for Birch.3.1 X-Size[9] Y-Size[9]\n",
      "[Thread] GaussianMixture.3.2 -> ML alg GaussianProcessRegressor\n",
      "\n",
      "[Thread] GaussianMixture.3.2 -> ML alg RandomForestRegressor\n",
      "[Thread] GaussianMixture.3.0 -> ML alg GaussianProcessRegressor\n",
      "[Thread] GaussianMixture.3.0 -> ML alg RandomForestRegressor\n",
      "[Thread] Birch.4.0 -> ML alg GaussianProcessRegressor\n",
      "[Thread] Birch.4.0 -> ML alg RandomForestRegressor\n",
      "[Thread] GaussianMixture.4.0 -> ML alg GaussianProcessRegressordata split for Birch.3.2 X-Size[10] Y-Size[10]\n",
      "[Thread] GaussianMixture.4.0 -> ML alg RandomForestRegressor\n",
      "[Thread] Birch.4.1 -> ML alg GaussianProcessRegressor\n",
      "[Thread] Birch.4.1 -> ML alg RandomForestRegressor\n",
      "[Thread] Birch.4.2 -> ML alg GaussianProcessRegressor\n",
      "\n",
      "[Thread] GaussianMixture.4.3 -> ML alg GaussianProcessRegressor\n",
      "data split for Birch.3.0 X-Size[5] Y-Size[5][Thread] GaussianMixture.4.3 -> ML alg RandomForestRegressor\n",
      "[Thread] Birch.4.2 -> ML alg RandomForestRegressor\n",
      "\n",
      "[Thread] Birch.3.1 -> ML alg GaussianProcessRegressor\n",
      "[Thread] Birch.3.1 -> ML alg RandomForestRegressor\n",
      "[Thread] Birch.3.2 -> ML alg GaussianProcessRegressor\n",
      "[Thread] Birch.3.0 -> ML alg GaussianProcessRegressor\n",
      "[Thread] Birch.3.0 -> ML alg RandomForestRegressor\n",
      "[Thread] Birch.3.2 -> ML alg RandomForestRegressor\n",
      "data split for MiniBatchKMeans.3.0 X-Size[9] Y-Size[9]\n",
      "data split for MiniBatchKMeans.3.1 X-Size[10] Y-Size[10]\n",
      "data split for MiniBatchKMeans.3.2 X-Size[5] Y-Size[5]\n",
      "[Thread] MiniBatchKMeans.3.0 -> ML alg GaussianProcessRegressor\n",
      "[Thread] MiniBatchKMeans.3.0 -> ML alg RandomForestRegressor\n",
      "[Thread] MiniBatchKMeans.3.1 -> ML alg GaussianProcessRegressor\n",
      "[Thread] MiniBatchKMeans.3.1 -> ML alg RandomForestRegressor[Thread] MiniBatchKMeans.3.2 -> ML alg GaussianProcessRegressor\n",
      "\n",
      "[Thread] MiniBatchKMeans.3.2 -> ML alg RandomForestRegressor\n",
      "data split for MiniBatchKMeans.4.0 X-Size[9] Y-Size[9]\n",
      "data split for MiniBatchKMeans.4.3 X-Size[5] Y-Size[5]\n",
      "data split for MiniBatchKMeans.4.2 X-Size[5] Y-Size[5]\n",
      "data split for MiniBatchKMeans.4.1 X-Size[5] Y-Size[5]\n",
      "[Thread] MiniBatchKMeans.4.0 -> ML alg GaussianProcessRegressor\n",
      "[Thread] MiniBatchKMeans.4.0 -> ML alg RandomForestRegressor\n",
      "[Thread] MiniBatchKMeans.4.3 -> ML alg GaussianProcessRegressor\n",
      "[Thread] MiniBatchKMeans.4.3 -> ML alg RandomForestRegressor\n",
      "[Thread] MiniBatchKMeans.4.2 -> ML alg GaussianProcessRegressor\n",
      "[Thread] MiniBatchKMeans.4.2 -> ML alg RandomForestRegressor[Thread] MiniBatchKMeans.4.1 -> ML alg GaussianProcessRegressor\n",
      "\n",
      "[Thread] MiniBatchKMeans.4.1 -> ML alg RandomForestRegressor\n",
      "data split for MiniBatchKMeans.2.0 X-Size[9] Y-Size[9]\n",
      "data split for MiniBatchKMeans.2.1 X-Size[15] Y-Size[15]\n",
      "[Thread] MiniBatchKMeans.2.0 -> ML alg GaussianProcessRegressor\n",
      "[Thread] MiniBatchKMeans.2.0 -> ML alg RandomForestRegressor[Thread] MiniBatchKMeans.2.1 -> ML alg GaussianProcessRegressor\n",
      "[Thread] MiniBatchKMeans.2.1 -> ML alg RandomForestRegressor\n",
      "\n",
      "Check R2 for GaussianMixture.3.0.GaussianProcessRegressor = 0.015984397927269356\n",
      "Check R2 for GaussianMixture.3.0.RandomForestRegressor = 11.492000000000274\n",
      "Check R2 for GaussianMixture.4.0.GaussianProcessRegressor = 0.015984397927269356\n",
      "Check R2 for GaussianMixture.4.0.RandomForestRegressor = 11.654000000000082\n",
      "Check R2 for GaussianMixture.4.3.GaussianProcessRegressor = 0.0015183686940645202\n",
      "Check R2 for GaussianMixture.4.3.RandomForestRegressor = 0.14550000000042473\n",
      "Check R2 for Birch.4.2.GaussianProcessRegressor = 0.015984397927269356\n",
      "Check R2 for Birch.4.2.RandomForestRegressor = 11.726000000000255\n",
      "Check R2 for GaussianMixture.4.1.GaussianProcessRegressor = 0.010515770759667475\n",
      "Check R2 for GaussianMixture.4.1.RandomForestRegressor = 0.30000000000012506\n",
      "Check R2 for Birch.3.0.GaussianProcessRegressor = 0.015984397927269356\n",
      "Check R2 for Birch.3.0.RandomForestRegressor = 11.672000000000281\n",
      "Check R2 for Birch.3.1.GaussianProcessRegressor = 3.447171039283603e-06\n",
      "Check R2 for Birch.3.1.RandomForestRegressor = 1.7319499999999892\n",
      "Check R2 for GaussianMixture.3.1.GaussianProcessRegressor = 3.447171039283603e-06\n",
      "Check R2 for GaussianMixture.3.1.RandomForestRegressor = 1.7850750000000715\n",
      "Check R2 for Birch.2.1.GaussianProcessRegressor = 3.447171039283603e-06\n",
      "Check R2 for Birch.2.1.RandomForestRegressor = 1.5989250000001618\n",
      "Check R2 for MiniBatchKMeans.3.2.GaussianProcessRegressor = 0.015984397927269356\n",
      "Check R2 for MiniBatchKMeans.3.2.RandomForestRegressor = 11.474000000000302\n",
      "Check R2 for MiniBatchKMeans.4.3.GaussianProcessRegressor = 0.03062884569649782\n",
      "Check R2 for MiniBatchKMeans.4.3.RandomForestRegressor = 3.321999999999747\n",
      "Check R2 for MiniBatchKMeans.4.2.GaussianProcessRegressor = 0.015984397927269356\n",
      "Check R2 for MiniBatchKMeans.4.2.RandomForestRegressor = 11.600000000000222\n",
      "Check R2 for MiniBatchKMeans.4.1.GaussianProcessRegressor = 0.024945696040759913\n",
      "Check R2 for MiniBatchKMeans.4.1.RandomForestRegressor = 0.8554999999994379\n",
      "Check R2 for Birch.3.2.GaussianProcessRegressor = 2.029971577712786e-05\n",
      "Check R2 for Birch.3.2.RandomForestRegressor = 2.4303999999996675\n",
      "Check R2 for Birch.4.1.GaussianProcessRegressor = 2.029971577712786e-05\n",
      "Check R2 for Birch.4.1.RandomForestRegressor = 2.493599999999765\n",
      "Check R2 for Birch.4.0.GaussianProcessRegressor = 3.447171039283603e-06\n",
      "Check R2 for Birch.4.0.RandomForestRegressor = 1.7090000000000884\n",
      "Check R2 for GaussianMixture.2.1.GaussianProcessRegressor = 3.447171039283603e-06\n",
      "Check R2 for GaussianMixture.3.2.GaussianProcessRegressor = 2.029971577712786e-05Check R2 for GaussianMixture.2.1.RandomForestRegressor = 1.2854500000000741\n",
      "Check R2 for GaussianMixture.3.2.RandomForestRegressor = 2.354999999999859\n",
      "\n",
      "Check R2 for GaussianMixture.4.2.GaussianProcessRegressor = 2.029971577712786e-05\n",
      "Check R2 for GaussianMixture.4.2.RandomForestRegressor = 2.509999999999775\n",
      "Check R2 for MiniBatchKMeans.3.0.GaussianProcessRegressor = 3.447171039283603e-06\n",
      "Check R2 for MiniBatchKMeans.3.0.RandomForestRegressor = 1.3922500000000013\n",
      "Check R2 for GaussianMixture.2.0.GaussianProcessRegressor = 6.129511019383114e-08\n",
      "Check R2 for GaussianMixture.2.0.RandomForestRegressor = 7.669232142857233\n",
      "Check avg_r2 for GaussianMixture.2 = 1.7542330747387171e-06\n",
      "Check avg_r2 for GaussianMixture.3 = 0.005336048271361923\n",
      "Check avg_r2 for GaussianMixture.4 = 0.0070097092741946195\n",
      "Check R2 for Birch.2.0.GaussianProcessRegressor = 6.129511019383114e-08\n",
      "Check R2 for MiniBatchKMeans.3.1.GaussianProcessRegressor = 2.029971577712786e-05\n",
      "Check R2 for MiniBatchKMeans.3.1.RandomForestRegressor = 2.3127999999996858\n",
      "Check R2 for Birch.2.0.RandomForestRegressor = 7.973633928571865\n",
      "Check avg_r2 for Birch.2 = 1.7542330747387171e-06\n",
      "Check avg_r2 for Birch.3 = 0.005336048271361923\n",
      "Check avg_r2 for Birch.4 = 0.005336048271361923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check R2 for MiniBatchKMeans.4.0.GaussianProcessRegressor = 3.447171039283603e-06Check R2 for MiniBatchKMeans.2.0.GaussianProcessRegressor = 3.447171039283603e-06\n",
      "Check R2 for MiniBatchKMeans.4.0.RandomForestRegressor = 1.6820250000000982\n",
      "\n",
      "Check R2 for MiniBatchKMeans.2.0.RandomForestRegressor = 1.786725000000152\n",
      "Check R2 for MiniBatchKMeans.2.1.GaussianProcessRegressor = 6.129511019383114e-08\n",
      "Check R2 for MiniBatchKMeans.2.1.RandomForestRegressor = 7.623705357143145\n",
      "Check avg_r2 for MiniBatchKMeans.2 = 1.7542330747387171e-06\n",
      "Check avg_r2 for MiniBatchKMeans.3 = 0.005336048271361923\n",
      "Check avg_r2 for MiniBatchKMeans.4 = 0.017890596708891593\n",
      "!!! An ML model family was selected !!!\n",
      "[array([661.]), array([696.3])]\n",
      "Score = 0.9856248517727629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_regression.py:582: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_regression.py:582: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "##########################################\n",
    "# Step 1. Machine Learning\n",
    "x_data = np.array([[667., 7], [693.3, 7], [732.9, 6], [658.9, 1], [702.8, 7], [667., 7], [693.3, 7], [732.9, 6], [658.9, 1], [702.8, 7], [667. , 7], [693.3, 7], [732.9, 6], [658.9, 1], [702.8, 7], [697.2, 1], [658.7, 2], [723.1, 1], [719.5, 3], [687.4, 1], [704.1, 1], [658.8, 4], [667.8, 3], [703.4, 3]])\n",
    "y_data = np.array([[667.], [693.3], [732.9], [658.9], [702.8], [667.], [693.3], [732.9], [658.9], [702.8], [667.], [693.3], [732.9], [658.9], [702.8],[697.2], [658.7], [723.1], [719.5], [687.4], [704.1], [658.8], [667.8], [703.4]])\n",
    "\n",
    "x_df = pd.DataFrame({'X1': x_data[:, 0], 'X2': x_data[:, 1]})\n",
    "y_df = pd.DataFrame({'Y': y_data[:, 0]})\n",
    "\n",
    "clustering_algs = {'GaussianMixture':[2, 3, 4], 'Birch':[2, 3, 4], 'MiniBatchKMeans':[2, 3, 4]}\n",
    "prediction_algs = ['GaussianProcessRegressor', 'RandomForestRegressor']\n",
    "selected_variables = ['X1']\n",
    "\n",
    "dc_ml = DataClusterBasedMachineLearning(x_df, y_df, selected_variables, clustering_algs, prediction_algs)\n",
    "ml_family = dc_ml.run()\n",
    "\n",
    "\n",
    "##########################################\n",
    "# Step 2. Prediction\n",
    "x_test_data = np.array([[661., 7], [696.3, 8]])\n",
    "y_test_data = np.array([[662.], [699.3]])\n",
    "x_test_df = pd.DataFrame({'X1': x_test_data[:, 0], 'X2': x_test_data[:, 1]})\n",
    "y_test_df = pd.DataFrame({'Y': y_test_data[:, 0]})\n",
    "\n",
    "yPredicted, score, y_label = dc_ml.perform_prediction(x_test_df, y_test_df)\n",
    "\n",
    "print(yPredicted)\n",
    "print(f'Score = {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
